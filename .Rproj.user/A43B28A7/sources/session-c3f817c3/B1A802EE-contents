---
title: "Graded Problem Set"
date: "ECO374"
output: pdf_document
urlcolor: blue
geometry: margin=0.75in
---

Install and load required R packages

```{r, message=FALSE, warning=FALSE}
if (!require("quantmod")) install.packages("quantmod")
if (!require("ggplot2")) install.packages("ggplot2")
if (!require("stats")) install.packages("stats")
if (!require("tsDyn")) install.packages("tsDyn")
if (!require("forecast")) install.packages("forecast")

library(quantmod) # functions: getSymbols
library(ggplot2)  # functions: ggplot
library(stats)    # functions: arima
library(tsDyn)    # functions: SETAR
library(forecast) # functions: auto.arima, nnetar
```

# 1. Data

1: Load Data

```{r}
df <- read.csv("TLNRESCONS.csv")
df$DATE <- as.Date(df$DATE, format = "%Y-%m-%d")
```

2: Plot data

```{r, fig.width=7, fig.height=2.5}
ggplot(df, aes(x=DATE, y=TLNRESCONS)) + geom_line(color="springgreen4") + 
  labs(x="Date", y="Construction Spending", title="U.S. Nonresidential Construction") + 
  theme_minimal() + theme(plot.title = element_text(size=10)) +
  scale_x_date(date_breaks="2 years", date_labels = "%Y") 
```

3: Difference data and plot

```{r, fig.width=7, fig.height=2.5}
D_df <- cbind(df, c(0, DIFF = diff(df$TLNRESCONS, lag=1, differences=1)))
colnames(D_df) <- c("DATE", "TLNRESCONS", "DIFF")
ggplot(D_df, aes(x=DATE, y=DIFF)) + geom_line(color="darkblue") + 
  labs(x="", y="", title="Differenced NASDAQ Composite Index, Close") + 
  theme_minimal() + theme(plot.title = element_text(size=10)) +
  scale_x_date(date_breaks="2 years", date_labels = "%Y-%m") 
```

4: Plot ACF of differenced data

```{r, fig.width=6, fig.height=3}
par(mar=c(4,4,0.5,0)) # set margin sizes
ACF <- acf(D_df$DIFF, lag.max=20, plot=FALSE, demean=TRUE)
plot(ACF[1:20], main="", cex.lab=0.75, cex.axis=0.75, xaxt="n")
axis(1,at=ACF$lag, cex.axis=0.75) # put a label at each lag value
```

Plot PACF of differenced data

```{r, fig.width=6, fig.height=3}
par(mar=c(4,4,0.5,0)) # set margin sizes
PACF <- pacf(D_df$DIFF, lag.max=20, plot=FALSE, demean=TRUE)
plot(PACF[1:20], main="", cex.lab=0.75, cex.axis=0.75, xaxt="n")
axis(1,at=PACF$lag, cex.axis=0.75) # put a label at each lag value
```

# 2. Model Selection

5: Best-fitting ARMA model on the full data set

```{r}
auto.arima(D_df$TLNRESCONS)
```

Best-fitting ARMA model on the differenced full data set

```{r}
auto.arima(D_df$DIFF)
```

6: Cross Validation:

Defining a function to get the mse of a neural net with given hyper parameters:

```{r}
TSCV_nnetar <- function(data, p, P, size) {
  TT <- length(data)
  T1 <- floor(TT/5) # start at 20% of the sample size
  step <- 20 # forecast horizon for MSE
  MSE.t <- matrix(0,nrow=TT-T1+1,ncol=1) # initialize
  y.hat <- MSE.t # initialize
  tseq <- seq(from=T1, to=TT, by=step)
  tseq <- tseq[-length(tseq)]
  for (j in tseq) {
    #print(j) # display progress through data
    set.seed(seed)
    nnetar.model <- nnetar(y=data[1:j-1], p=p, P=P, size=size) # fit nnetar model on the training set
    NN.f <- forecast(nnetar.model,h=step) # generate forecast
    y.hat <- as.numeric(NN.f$mean)
    js <- j+step-1
    MSE.t[(j-T1+1):(js-T1+1)] <- (as.numeric(data[j:js])-y.hat)^2
  }
  MSE.validation <- mean(MSE.t)
  return(MSE.validation)
}
```

Run this function for 80 different combinations of hyper parameters to find the optimal combination

```{r}
size.max <- 8
lag.max <- 10
TSCV <- matrix(0, nrow=lag.max, ncol=size.max)
for (s in 1:size.max) {
  for (k in 1:lag.max) {
    TSCV[k,s] <- TSCV_nnetar(data=D_df$DIFF, p=k, P=0, size=s)
    #writeLines(paste("Size = ", s, "Lag = ", k, "  Validation MSE = ", TSCV[k,s]))
  }}
TSCV <- data.frame(TSCV)
colnames(TSCV) <- paste("hn.", 1:size.max , sep="")
```

Plot validation MSE:

```{r, fig.width=7, fig.height=5}
colors <- c("hidden.neurons.1"="blue", "hidden.neurons.2"="red", 
            "hidden.neurons.3"="darkgreen", "hidden.neurons.4"="cyan",
            "hidden.neurons.5"="darkviolet", "hidden.neurons.6"="gold",
            "hidden.neurons.7"="pink", "hidden.neurons.8"="tan")
ggplot() + 
  geom_line(data=TSCV, aes(x=index(TSCV), y=hn.1, color="hidden.neurons.1")) +
  geom_line(data=TSCV, aes(x=index(TSCV), y=hn.2, color="hidden.neurons.2")) +
  geom_line(data=TSCV, aes(x=index(TSCV), y=hn.3, color="hidden.neurons.3")) +  
  geom_line(data=TSCV, aes(x=index(TSCV), y=hn.4, color="hidden.neurons.4")) +  
  geom_line(data=TSCV, aes(x=index(TSCV), y=hn.5, color="hidden.neurons.5")) +  
  geom_line(data=TSCV, aes(x=index(TSCV), y=hn.6, color="hidden.neurons.6")) +
  geom_line(data=TSCV, aes(x=index(TSCV), y=hn.7, color="hidden.neurons.7")) +  
  geom_line(data=TSCV, aes(x=index(TSCV), y=hn.8, color="hidden.neurons.8")) +    
  geom_point(data=TSCV, aes(x=index(TSCV), y=hn.1, color="hidden.neurons.1")) + 
  geom_point(data=TSCV, aes(x=index(TSCV), y=hn.2, color="hidden.neurons.2")) +
  geom_point(data=TSCV, aes(x=index(TSCV), y=hn.3, color="hidden.neurons.3")) +  
  geom_point(data=TSCV, aes(x=index(TSCV), y=hn.4, color="hidden.neurons.4")) +  
  geom_point(data=TSCV, aes(x=index(TSCV), y=hn.5, color="hidden.neurons.5")) +  
  geom_point(data=TSCV, aes(x=index(TSCV), y=hn.6, color="hidden.neurons.6")) +
  geom_point(data=TSCV, aes(x=index(TSCV), y=hn.7, color="hidden.neurons.7")) +    
  geom_point(data=TSCV, aes(x=index(TSCV), y=hn.8, color="hidden.neurons.8")) +      
  labs(x="Lag", y="", title="Time Series Validation MSE") +
  theme_minimal() + 
  theme(plot.title = element_text(size=10)) +
  theme(legend.position = c(0.3, 0.75)) +   
  scale_color_manual(name="", values=colors) + 
  scale_x_continuous(breaks=index(TSCV), labels=paste(index(TSCV)))
```

It seems based on this cross validation that the optimal combination is 10 lags with 6 neurons, since this gives us the smallest MSE

```{=tex}
\newpage
7
```
: Time-series validation MSE for Specified Models

```{r, warning=FALSE}
data <- D_df$TLNRESCONS
D_data <- D_df$DIFF

# Loop over different model specifications
for (m in 1:6) {
  
  TT <- length(data)
  T1 <- floor(0.2*TT) # start at 20% of the sample size
  step <- 20 # forecast data horizon for MSE
  MSE.t <- matrix(0,nrow=TT-T1+1,ncol=1) # initialize
  MAE.t <- MSE.t
  MAPE.t<- MSE.t
  tseq <- seq(from=T1, to=TT, by=step)
  tseq <- tseq[-length(tseq)]

  for (j in tseq) {

    # auto.arima model selected earlier
    if (m==1) {fcst <- forecast(arima(D_data[1:j-1], order=c(1,0,1)), h=step)
               yhat <- as.numeric(fcst[[4]])}    
    
    # ARMA on differenced data with order chosen based on acf and pacf
    if (m==2) {fcst <- forecast(arima(D_data[1:j-1], order=c(4,0,3)), h=step)
              # the fcst$mean forecast is stored in the 4th element of the list fcst
               yhat <- as.numeric(fcst[[4]])}
    
    # ARMA on differenced data with seasonal component
    if (m==3) {fcst <- forecast(arima(data[1:j-1], order=c(4,1,3), seasonal=c(1, 0, 1)), h=step)
               yhat <- as.numeric(fcst[[4]])}

    # SETAR model with a threshold of 100
    if (m==4) {fcst <- predict(setar(D_data[1:j-1], mL=1, mH=1, th=0), n.ahead=step)
               yhat <- as.numeric(fcst)
               yhat <- cumsum(yhat) + as.numeric(last(data[1:j-1]))} # cumulate forecast differences
               
    # LSTAR model
    if (m==5) {fcst <- predict(lstar(D_data[1:j-1], m=1, d=1, mL=1, mH=1, gamma=1, th=1, trace=FALSE), n.ahead=step)
               yhat <- as.numeric(fcst)
               yhat <- cumsum(yhat) + as.numeric(last(data[1:j-1]))} # cumulate forecast differences    
    
    # NNAR with parameters selected in file 5. NNAR Application
    if (m==6) {set.seed(seed)
               fcst <- forecast(nnetar(data[1:j-1], p=10, P=0, size=6), h=step)
               # the fcst$mean forecast is stored in the 16th element of the list fcst
               yhat <- as.numeric(fcst[[16]][1:step])}

    js <- j+step-1
    MSE.t[(j-T1+1):(js-T1+1)] <- (as.numeric(data[j:js])-yhat)^2
    MAE.t[(j-T1+1):(js-T1+1)] <- abs(as.numeric(data[j:js])-yhat)
    MAPE.t[(j-T1+1):(js-T1+1)] <- 100*abs((as.numeric(data[j:js])-yhat)/yhat)
  }

  if (m<=3) print(fcst$method)
  if (m==4) print("SETAR")
  if (m==5) print("LSTAR")  
  if (m==6) print("NNAR")  

  print(paste("MSE  =", mean(MSE.t)))
  print(paste("MAE  =", mean(MAE.t)))
  print(paste("MAPE =", mean(MAPE.t)))
  print(" ")
}
```

8.  The SETAR model has the smallest validation MSE and would therefore be our model of choice. The seasonal ARIMA model is also good, but the neural net had poor performance, which could indicate that it was overfitted.
